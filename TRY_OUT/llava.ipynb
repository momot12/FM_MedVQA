{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = 'eepy/datasets'\n",
    "hf_cache_path = 'eepy/hf-cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llava\n",
    "https://huggingface.co/docs/transformers/model_doc/llava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import requests\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d146faab254b398e9ca460b5ec374a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/70.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860474f6ce0b46999e49e536038bd76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b927cd88dd2c46a0b4decf7aa31441a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73789e77a5424089aea9821810500a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e31edcb8a146ca8c3912d579e1c3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea83f551ef1d407d8a161551604e2b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c565827b254a0591d458e6ea023e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url_llava = \"llava-hf/llava-1.5-7b-hf\"\n",
    "# processes img\n",
    "processor = AutoProcessor.from_pretrained(url_llava, cache_dir=hf_cache_path)\n",
    "# actual model \n",
    "model = LlavaForConditionalGeneration.from_pretrained(url_llava, cache_dir=hf_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question.\n",
      "Your question: What can you see on the images?\n"
     ]
    }
   ],
   "source": [
    "print('Enter your question.')\n",
    "user_question = input()\n",
    "print(f\"Your question: {user_question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption:\n",
      " \tUSER:   What can you see on the images? \n",
      "\tASSISTANT: The images show a close-up view of a person's abdomen, focusing on the stomach area. The stomach is visible, along with the spine and the ribcage. The images provide a detailed perspective of the internal organs and their surrounding structures. \n",
      "\n",
      "Generated Caption:\n",
      " \tUSER:   What can you see on the images? \n",
      "\tASSISTANT: The image shows a close-up view of a person's back, focusing on the spine and the area around the spine. There are two arrows pointing towards the spine, indicating the location of the image. The image is in black and white, giving it a classic or vintage appearance. \n",
      "\n",
      "Generated Caption:\n",
      " \tUSER:   What can you see on the images? \n",
      "\tASSISTANT: The images show a close-up view of a person's lungs, specifically focusing on the right lung. The lungs are visible in the chest area, and the right lung is shown in detail, highlighting its structure and function. The images provide a clear and detailed perspective of the human respiratory system. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample img from internet\n",
    "#url_img = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "#image = Image.open(requests.get(url_img, stream=True).raw)\n",
    "\n",
    "prompt = f\"\\tUSER: <image> {user_question} \\n\\tASSISTANT:\"\n",
    "\n",
    "path_img = \"/mount/studenten/team-lab-cl/data2024/fm_med_vqa/momo_fm/FM_MedVQA/sample_datasets/ROCOv2/train/\"\n",
    "imgs = [\"ROCOv2_2023_train_000307.jpg\", \"ROCOv2_2023_train_001490.jpg\", \"ROCOv2_2023_train_060077.jpg\"]\n",
    "\n",
    "for img in imgs:\n",
    "    image = Image.open(path_img+img) \n",
    "    caption = \"Describe the scene in this image.\"\n",
    "\n",
    "    # Prepare the inputs\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate predictions\n",
    "    generate_ids = model.generate(**inputs, max_new_tokens=80)\n",
    "    caption = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    # Display the generated caption\n",
    "    print(\"Generated Caption:\\n\", caption, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of trial runs:\n",
    "#### Original captions: \n",
    "Img1 ROCOv2_2023_train_000307.jpg: *CT scan with sagittal view of inferior vena cava thrombus*   \n",
    "Img2 ROCOv2_2023_train_001490.jpg: *Barium swallow examination showing evidence of multiple webs in the upper esophagus (arrows).*   \n",
    "Img3 ROCOv2_2023_train_060077.jpg: *Chest computed tomography showing a right lung nodule (arrow).*  \n",
    "\n",
    "\n",
    "#### Question 1: Is the body part healthy?  \n",
    "Notes: max_new_tokens=30, to second sentence is cut for 2nd sample.  \n",
    "Model Output:  \n",
    "\n",
    "    Generated Caption:\n",
    "        USER:   Is the body part healthy? \n",
    "        ASSISTANT: No \n",
    "\n",
    "    Generated Caption:\n",
    "        USER:   Is the body part healthy? \n",
    "        ASSISTANT: No, the body part is not healthy. The image shows a close-up of a person's back, with a large, curved \n",
    "\n",
    "    Generated Caption:\n",
    "        USER:   Is the body part healthy? \n",
    "        ASSISTANT: No\n",
    "\n",
    "\n",
    "#### Question 2: Which body part is seen?\n",
    "Notes: same question, different run gave 'abdomen' instead of 'stomach'.    \n",
    "Model output:  \n",
    "\n",
    "    Generated Caption:  \n",
    "        USER:   Which body part is seen?   \n",
    "        ASSISTANT: Stomach   \n",
    "\n",
    "    Generated Caption:  \n",
    "        USER:   Which body part is seen?   \n",
    "        ASSISTANT: Spine   \n",
    "\n",
    "    Generated Caption:  \n",
    "        USER:   Which body part is seen?   \n",
    "        ASSISTANT: Lung   \n",
    "\n",
    "\n",
    "#### Question 3: What can you see on the images? \n",
    "Notes: max_new_tokens=80  \n",
    "Model output:  \n",
    "\n",
    "    Generated Caption:  \n",
    "        USER:   What can you see on the images?   \n",
    "        ASSISTANT: The images show a close-up view of a person's abdomen, focusing on the stomach area. The stomach is visible, along with the spine and the ribcage. The images provide a detailed perspective of the internal organs and their surrounding structures. \n",
    "\n",
    "    Generated Caption:  \n",
    "        USER:   What can you see on the images?   \n",
    "        ASSISTANT: The image shows a close-up view of a person's back, focusing on the spine and the area around the spine. There are two arrows pointing towards the spine, indicating the location of the image. The image is in black and white, giving it a classic or vintage appearance. \n",
    "\n",
    "    Generated Caption:  \n",
    "        USER:   What can you see on the images?   \n",
    "        ASSISTANT: The images show a close-up view of a person's lungs, specifically focusing on the right lung. The lungs are visible in the chest area, and the right lung is shown in detail, highlighting its structure and function. The images provide a clear and detailed perspective of the human respiratory system. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_vqa_py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
